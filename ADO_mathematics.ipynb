{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADO for CDN\n",
    "\n",
    "We used the [ADOpy](https://github.com/adopy/adopy) to inspire our development of ADO for CDN. In this notebook we go over the mathematical foundation underlying ADO.\n",
    "\n",
    "## Introduction to grid-based ADO algorithm\n",
    "\n",
    "ADO works by first pre-computing the log-likelihood, entropy, and prior at step 0. Then the optimization takes place iteratively over three steps by updating the mutual information after a choice is made. This theoretical framework will be written in general for any given task design. In practice, a lot of computations can be written generally to work for any task design. The initial computation of the log-likelihood is based on each task and $SV$ defined by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Pre-computation\n",
    "\n",
    "In each task, or experiment, the participant makes a decision or a choice, $y$. This data is fit to a model defined by parameters, $\\theta$. Finally, the design or choices presented to the participant is defined by the choice set space, $d$. In the code, these three values are defined first to subsequently compute the following distributions, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:yellow\">1. Precompute the log-likelihood $log(p(y|\\theta,d))$ for all values in $y$,$\\theta$, and $d$:</span>.\n",
    "\n",
    "\n",
    "Given each possible combination ($y_i$,$\\theta_j$,$d_k$), compute $SV_{reward}$ and $SV_{null}$ defined by the model. Use the difference between the two $\\Delta SV = SV_{reward} - SV_{null}$ to get a probability, $p_l$, given a $\\gamma_j$ (taken from $\\theta_j$). Finally compare probability $p_l$ to choice $y_i$ and compute the $log(p(y_i|\\theta_j,d_k))$. In Python you can compute `log_lik` using `choice` as $y_i$ `p_choose_reward` as $p_l$ by hand or using `bernoulli.logpmf` from `scipy.stats`:\n",
    "\n",
    "`log_lik = (choice==1)*np.log(p_choose_reward) + (choice==0)*np.log(1-p_choose_reward)`\n",
    "\n",
    "`log_lik = bernoulli.logpmf(choice, p_choose_reward)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Precompute the entropy $H(Y(d)|\\theta) = -\\sum_{y} p(y|\\theta,d) log(p(y|\\theta,d))$ for all values of $d$ and $\\theta$:\n",
    "\n",
    "We use the log-likelihood (`log_lik`) to compute entropy as the second term in the summation is the `log_lik`, while the first term is the likelihood, so we exponentiate $p(y|\\theta,d) = e^{log(p(y|\\theta,d))}$. In the code this is computed using Numpy's function [`np.einsum()`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)\n",
    "\n",
    "`ent = -1 * np.einsum('dpy,dpy->dp', np.exp(log_lik), log_lik)`\n",
    "\n",
    "Briefly, the function `np.einsum()` uses Einstein summation convention to sum over $y$. The first term is a string indicating what axis you intend to sum over. In this case `dpy` refers to design, parameters, output/response. Then the notation used `dpy,dpy-> dp` indicates summing along $y$, axis=$2$ from $(0,1,2)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize prior $p_t(\\theta)$ for each discretized values of $\\theta$:\n",
    "\n",
    "If you have no priors then you can set this to a uniform distribution, normalized to $1.0$ as follows:\n",
    "\n",
    "`log_prior = np.log(np.ones(n_p, dtype=np.float32) / n_p)`\n",
    "\n",
    "where `n_p` is the number of parameters ($\\theta_j$). If you have a `prior` that you want to initialize, you can define an array with zeros, then set the parameter index you want to initialize to 1.0. Then convert this prior to a `log_prior` by trapping the prior into a `log_prior`. This is part of the code paraphrased here for illustration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_prior is :\n",
      " [-1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -5.9604645e-08 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01 -1.6118095e+01 -1.6118095e+01\n",
      " -1.6118095e+01 -1.6118095e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# here we think parameter index 12 is the one we want to initialize\n",
    "pmid_idx = 12\n",
    "n_p = 30 # this is number of parameters computed elsewhere\n",
    "prior = np.zeros(n_p, dtype=np.float32)\n",
    "prior[pmid_idx] = 1\n",
    "noise_ratio = 1e-7\n",
    "log_prior = np.log((1 - 2 * noise_ratio) * prior + noise_ratio)\n",
    "print('log_prior is :\\n {}'.format(log_prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Design Optimization\n",
    "\n",
    "After pre-computing the above distributions, we begin the optimization phase. In this first step, we compute several different distributions to allow us to compute the mutal information which is maximized to find the new design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the marginal likelihood $p(y|d) = \\sum_{\\theta} p(y|\\theta,d)p_t(\\theta)$ for all values of $y$ and $d$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. Compute cond ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Computer marg entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. ID optimal design to max MI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Experimentation\n",
    "\n",
    "Simulate or ask for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Bayesian Update\n",
    "\n",
    "1. Update posterior\n",
    "\n",
    "2. Set new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
